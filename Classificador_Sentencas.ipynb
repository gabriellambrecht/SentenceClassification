{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Classificador_Sentencas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabriellambrecht/SentenceClassification/blob/main/Classificador_Sentencas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFOTiqrtNvyy"
      },
      "source": [
        "# Rotinas de inicialização\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hkhc10wNrGt",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a75067e-368b-497c-f525-afcf2c356c99"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GoMP1SK7vqT"
      },
      "source": [
        "# Define qual dispositivo usará no treinamento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty3B_swT8hIs"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Se disponível usa o ambiente CUDA, senão usa o CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcdeAIOlHtMx"
      },
      "source": [
        "Define qual o tamanho máximo de frases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwbpeN_PMiu"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('dataset_feedback.csv', sep=',', header=None)\n",
        "dataset.columns = ['text', 'label']\n",
        "\n",
        "# Armazena em uma lista o tamanho de cada sentença\n",
        "lista_tamanho = [len(i.split()) for i in dataset['text']]\n",
        "\n",
        "# Monta e exibe o histograma do tamanho de cada sentença\n",
        "pd.Series(lista_tamanho).hist(bins = 16, grid=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lpCuXW-Ai5w"
      },
      "source": [
        "# Carrega dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4giRzM7NtHJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('dataset_feedback.csv', sep=',', header=None)\n",
        "dataset.columns = ['text', 'label']\n",
        "dataset.label = dataset.label+2\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfWnApvOoE7"
      },
      "source": [
        "# Divide dataset em conjutos de treino, validação e teste\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfhSPF5jOWb7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separa dataset original em dataset de treino e temporário\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['text'], dataset['label'], \n",
        "                                                                    random_state=2021, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=dataset['label'])\n",
        "# Separa dataset temporário em dataset de validação e teste\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2021, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7hsdLoCO7uB"
      },
      "source": [
        "# Import BERT Model and BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1kY3gZjO2RE"
      },
      "source": [
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Importa modelo Bertimbau pré-treinado em português\n",
        "bert = AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased', num_labels=5)\n",
        "\n",
        "# Importa tokenizer Bertimbau pré-treinamento em português\n",
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wIYaWI_Prg8"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk5S7DWaP2t6"
      },
      "source": [
        "# Define tamanho fixo das sentenças\n",
        "max_seq_len = 17\n",
        "\n",
        "def tokeniza_sentencas (sentencas):\n",
        "  return tokenizer.batch_encode_plus(\n",
        "          sentencas.tolist(),\n",
        "          max_length = max_seq_len,\n",
        "          padding='max_length',\n",
        "          truncation=True,\n",
        "          return_token_type_ids=False\n",
        "  )\n",
        "\n",
        "# Tokeniza as sentenças dos datasets de treinamento, validação e teste\n",
        "tokens_train = tokeniza_sentencas(train_text)\n",
        "tokens_val = tokeniza_sentencas(val_text)\n",
        "tokens_test = tokeniza_sentencas(test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsm8bkRZQTw9"
      },
      "source": [
        "# Convert Integer Sequences to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR-lXwmzQPd6"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Transforma em tensores o conjunto de treinamento\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# Transforma em tensores o conjunto de validação\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# Transforma em tensores o conjunto de teste\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1cOBlcRLuk"
      },
      "source": [
        "# Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUy9JKFYQYLp"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Define o batch size (tamanho de registro para cada iteração do treinamento)\n",
        "batch_size = 32\n",
        "\n",
        "# Monta um DataLoader do dataset de treinamento\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Monta um DataLoader do dataset de validação\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7ahGBUWRi3X"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3iEtGyYRd0A"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    # Define o método de inicialização\n",
        "    def __init__(self, bert):\n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert = bert \n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.relu =  nn.ReLU()\n",
        "      self.fc1 = nn.Linear(1024, 512)\n",
        "      self.fc2 = nn.Linear(512,5)\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # Define o método de avanço\n",
        "    def forward(self, sent_id, mask):\n",
        "      cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)[1]\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "      x = self.softmax(x)\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9CDpoMQR_rK"
      },
      "source": [
        "# Calcula os pesos de classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izY5xH5eR7Ur",
        "outputId": "704632ba-1587-4bfb-ff0b-276067e12683"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#Calcula os pesos de cada classe\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "print(class_wts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.62884615 1.52093023 2.10967742 1.45333333 0.62884615]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1WvfY2vSGKi"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Converte pesos das classes em tensor e armazena no dispositivo de execução\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# Configura classe de cálculo de perda usando os pesos \n",
        "cross_entropy  = nn.CrossEntropyLoss(weight=weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My4CA0qaShLq"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBAJJVuJRliv"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# Congela parâmetros não utilizados\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Instância o modelo com a fine-tuning e armazena no dispositivo de execução\n",
        "model = BERT_Arch(bert)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define a função de otimização\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rskLk8R_SahS"
      },
      "source": [
        "def train():\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for batch in train_dataloader:\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    model.zero_grad()        \n",
        "    preds = model(sent_id, mask)\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    total_loss = total_loss + loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXovFDlSxB5"
      },
      "source": [
        "def evaluate():\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  for batch in val_dataloader:\n",
        "    batch = [t.to(device) for t in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    with torch.no_grad():\n",
        "      preds = model(sent_id, mask)\n",
        "      loss = cross_entropy(preds,labels)\n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KZEgxRRTLXG"
      },
      "source": [
        "# Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1USGTntS3TS"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('\\nEpoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    train_loss = train()\n",
        "    valid_loss = evaluate()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    print(f'Training Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4SVftkkTZXA"
      },
      "source": [
        "# Get Predictions for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZl0SZmFTRQA"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Carrega o estado do melhor modelo treinado\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "\n",
        "# Busca previsões para o dataset de teste\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "  \n",
        "# Imprime o relatório de classificação comparando as previsões e os valores reais\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqzLS7rHTp4T"
      },
      "source": [
        "pd.crosstab(test_y, preds)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}